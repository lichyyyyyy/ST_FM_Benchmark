{"cells":[{"metadata":{"id":"b33599b14c7eacb4"},"cell_type":"markdown","source":["# Unsupervised Finetune scGPT-spatial\n","\n","Object: GEPS intra, GEPS inter\n","\n","Requirements: python <= 3.10, torch==2.3.0+cu121, torchtext==0.18.0, numpy<1.24"],"id":"b33599b14c7eacb4"},{"cell_type":"markdown","source":["## Colab Pre-Requisites"],"metadata":{"id":"3cA4OTRRuPQN"},"id":"3cA4OTRRuPQN"},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","\n","# mount to google drive\n","from google.colab import drive\n","# drive.flush_and_unmount()\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ST_FM_Benchmark/scGPT_spatial"],"metadata":{"id":"j88tbUJruMCD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758311413653,"user_tz":420,"elapsed":22335,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}},"outputId":"f2c8865b-ddf4-4417-dfe5-9010f32f4303"},"id":"j88tbUJruMCD","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ST_FM_Benchmark/scGPT_spatial\n"]}]},{"cell_type":"code","source":["# # torchtext only support torch 2.3; torch text 0.18.0 is latest version.\n","# # https://pytorch.org/get-started/locally\n","# ! pip install torch==2.3.0+cu121 torchvision==0.18.0 --index-url https://download.pytorch.org/whl/cu121\n","# ! pip install torchtext==0.18.0\n","# ! pip install scgpt\n","# ! pip install datasets\n","# ! pip install scanpy\n","# ! pip install tdigest\n","# ! pip install anndata\n","# ! pip install numpy==1.23.5"],"metadata":{"id":"jKrV2lvPu0-M","executionInfo":{"status":"ok","timestamp":1758311413702,"user_tz":420,"elapsed":47,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"id":"jKrV2lvPu0-M","execution_count":3,"outputs":[]},{"cell_type":"code","source":["# verify torch version\n","# expect:\n","# Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","# Torch: 2.3.0+cu121 CUDA: 12.1\n","# numpy:  1.23.5\n","# CXX11_ABI: False\n","\n","# import sys, torch, numpy as np\n","# print(\"Python:\", sys.version)\n","# print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n","# print(\"numpy: \", np.__version__)\n","# try:\n","#     print(\"CXX11_ABI:\", torch._C._GLIBCXX_USE_CXX11_ABI)  # 0=FALSE, 1=TRUE\n","# except Exception as e:\n","#     print(e)\n"],"metadata":{"id":"Zz3JLALEx1Xw","executionInfo":{"status":"ok","timestamp":1758311413706,"user_tz":420,"elapsed":2,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"id":"Zz3JLALEx1Xw","execution_count":4,"outputs":[]},{"metadata":{"id":"c8f0b2e98e4b6dfb"},"cell_type":"markdown","source":["## Helper Functions"],"id":"c8f0b2e98e4b6dfb"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:54.211620Z","start_time":"2025-09-19T11:03:53.209372Z"},"id":"f889fb5ac1efdad2","executionInfo":{"status":"ok","timestamp":1758311416675,"user_tz":420,"elapsed":2965,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["import warnings\n","from dataclasses import dataclass\n","from typing import List, Tuple  # type: ignore\n","\n","import torch\n","\n","warnings.filterwarnings('ignore')\n","\n","\n","# configs\n","@dataclass\n","class Config:\n","    h5ad: str = \"\"\n","    ckpt_dir: str = \"\"\n","    finetuned_ckpt_dir: str = \"results\"\n","\n","    seed: int = 42\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    # hyperparams\n","    n_hvg: int = 600\n","    n_bins: int = 51\n","    max_seq_len: int = 601  # n_hvg + 1 for <cls>\n","    mask_ratio: float = 0.4\n","    pad_token: str = '<pad>'\n","    pad_value: int = -2\n","    mask_value: int = -1\n","    lr: float = 5e-05\n","    schedule_ratio = 0.9  # Default rate for learning rate decay\n","    epochs: int = 15\n","    batch_size: int = 16\n","    n_neighbors: int = 7  # Neighbors per patch. Each patch contains (1+n_neighbors) cells.\n","\n","    # data\n","    batch_key_col: str = \"protocol\"  # batch key col name in adata.obs\n","    gene_names_col: str = \"gene_names\"  # gene name col in adata.var. Use `index` if gene name is the index of adata.var.\n","    coordinates_x_col: str = \"array_row\"  # x coordinates in adata.obs.\n","    coordinates_y_col: str = \"array_col\"  # x coordinates in adata.obs.\n","\n","    # domain prediction\n","    dom_key: str = \"domain_scgpt\"  # key for the domain prediction results\n","    rep_key: str = \"X_scGPT\"  # key for the embeddings\n","    method: str = \"leiden\"  # \"leiden\" or \"louvain\"\n","    target_clusters: int = 6\n","    leiden_resolutions: Tuple[float, ...] = (0.3, 0.5, 0.8, 1.0, 1.2)"],"id":"f889fb5ac1efdad2","outputs":[],"execution_count":5},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.530934Z","start_time":"2025-09-19T11:03:54.352573Z"},"id":"ea6018990d3c3721","outputId":"94c2ae26-c56d-4885-dee3-38ba11730bf2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758311449252,"user_tz":420,"elapsed":32574,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["from scgpt_spatial.tasks.cell_emb import load_pretrained\n","from pathlib import Path\n","from scgpt.tokenizer import GeneVocab\n","import scgpt_spatial\n","import json\n","\n","\n","def load_backbone_and_vocab(ckpt_dir: Path, device: str) -> tuple[\n","    scgpt_spatial.model.TransformerModel, GeneVocab, dict]:\n","    model_config_file = ckpt_dir / \"args.json\"\n","    model_file = ckpt_dir / \"best_model.pt\"\n","    vocab_file = ckpt_dir / \"vocab.json\"\n","    with open(model_config_file, \"r\") as f:\n","        model_args = json.load(f)\n","    vocab = GeneVocab.from_file(vocab_file)\n","    for tok in (\"<pad>\", \"<cls>\", \"<eoc>\"):\n","        if tok not in vocab:\n","            vocab.append_token(tok)\n","    vocab.set_default_index(vocab[\"<pad>\"])\n","\n","    model = scgpt_spatial.model.TransformerModel(\n","        ntoken=len(vocab),\n","        d_model=model_args[\"embsize\"],\n","        nhead=model_args[\"nheads\"],\n","        d_hid=model_args[\"d_hid\"],\n","        nlayers=model_args[\"nlayers\"],\n","        nlayers_cls=model_args[\"n_layers_cls\"],\n","        n_cls=1,\n","        vocab=vocab,\n","        dropout=model_args[\"dropout\"],\n","        pad_token=model_args[\"pad_token\"],\n","        pad_value=model_args[\"pad_value\"],\n","        do_mvc=True,\n","        do_dab=False,\n","        use_batch_labels=True,\n","        num_batch_labels=4,  # trained by 4 protocols\n","        input_emb_style=model_args[\"input_emb_style\"],\n","        n_input_bins=model_args[\"n_bins\"],\n","        cell_emb_style='cls',\n","        mvc_decoder_style=\"inner product\",\n","        ecs_threshold=0.8,\n","        explicit_zero_prob=False,\n","        use_generative_training=True,\n","        use_fast_transformer=False,\n","        fast_transformer_backend=\"flash\",\n","        pre_norm=False,\n","        use_MVC_impute=True,\n","        impute_MVC_knn_k=model_args[\"impute_k\"],\n","        use_moe_dec=True,\n","    )\n","    load_pretrained(model, torch.load(model_file, map_location=device),\n","                    verbose=True)\n","    model.to(device)\n","    model.eval()\n","\n","    # sync config params\n","    config.lr = model_args[\"lr\"]\n","    return model, vocab, model_args"],"id":"ea6018990d3c3721","outputs":[{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","/usr/local/lib/python3.11/dist-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n","  warnings.warn(\"flash_attn is not installed\")\n","/usr/local/lib/python3.11/dist-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n","  warnings.warn(\"flash_attn is not installed\")\n","<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:1047: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:1047: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n"]}],"execution_count":6},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.549350Z","start_time":"2025-09-19T11:03:56.538621Z"},"id":"ea1e4f07ab05442c","executionInfo":{"status":"ok","timestamp":1758311449267,"user_tz":420,"elapsed":17,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from scipy.sparse import issparse\n","import scanpy as sc\n","\n","\n","def split_dataset(h5ad_path: Path, config: Config, vocab: GeneVocab):\n","    adata = sc.read_h5ad(str(h5ad_path))\n","\n","    # tokenize genes\n","    adata.var[\"id_in_vocab\"] = [\n","        vocab[gene] if gene in vocab else -1 for gene in (adata.var.index if\n","                                                          config.gene_names_col == 'index' else \\\n","                                                              adata.var[\n","                                                                  config.gene_names_col])\n","    ]\n","    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n","    genes = adata.var.index.tolist() if config.gene_names_col == 'index' else \\\n","        adata.var[config.gene_names_col].tolist()\n","    gene_ids = np.array(vocab(genes), dtype=int)\n","\n","    all_counts = (\n","        adata.X.toarray() if issparse(adata.X) else adata.X\n","    )\n","\n","    batch_ids = adata.obs[config.batch_key_col].tolist()\n","    batch_ids = np.array(batch_ids)\n","\n","    coordinates = pd.concat(\n","        [adata.obs[config.coordinates_x_col],\n","         adata.obs[config.coordinates_y_col]],\n","        axis=1).to_numpy()\n","\n","    (\n","        train_data,\n","        valid_data,\n","        train_batch_labels,\n","        valid_batch_labels,\n","        train_xy,\n","        valid_xy,\n","    ) = train_test_split(\n","        all_counts, batch_ids, coordinates, test_size=0.1, shuffle=True\n","    )\n","\n","    scgpt_spatial.logger.info(\n","        f\"Train set number of samples: {train_data.shape[0]}  Valid set number of samples: {valid_data.shape[0]}\"\n","    )\n","\n","    return gene_ids, train_data, valid_data, train_batch_labels, valid_batch_labels, train_xy, valid_xy"],"id":"ea1e4f07ab05442c","outputs":[],"execution_count":7},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.561419Z","start_time":"2025-09-19T11:03:56.555916Z"},"id":"b8b515a2c13dd7b7","executionInfo":{"status":"ok","timestamp":1758311449282,"user_tz":420,"elapsed":13,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","\n","# dataset. Adopted from scgpt-spatial.\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, count_matrix, gene_ids, batch_labels, coordinates,\n","                 config: Config):\n","        self.slide_mean = np.mean(\n","            count_matrix[count_matrix.nonzero()[0], count_matrix.nonzero()[1]])\n","        self.count_matrix = count_matrix / self.slide_mean\n","        self.gene_ids = gene_ids\n","        self.batch_ids = batch_labels\n","        self.coordinates = coordinates\n","        self.gene_stats_dict = pd.read_csv(\n","            Path(config.ckpt_dir, \"all_dict_mean_std.csv\"), index_col=0)\n","        self.config = config\n","        # For any new genes not already in gene_stats_dict\n","        # Calculate mean from current dataset\n","        new_genes = set(self.gene_ids).difference(\n","            set(self.gene_stats_dict.index.values))\n","        for i in new_genes:\n","            idx = np.where(self.gene_ids == i)[0]\n","            col = self.count_matrix[:, idx].flatten()\n","            nonzero_idx = np.nonzero(col)[0]\n","            values = col[nonzero_idx]\n","            self.gene_stats_dict.loc[i] = [float(values.mean())]\n","\n","    def __len__(self):\n","        return len(self.count_matrix)\n","\n","    def __getitem__(self, idx):\n","        row = self.count_matrix[idx]\n","        nonzero_idx = np.nonzero(row)[0]\n","        # values = row[nonzero_idx]\n","        # genes = self.gene_ids[nonzero_idx]\n","        mean_divide_by = self.gene_stats_dict.loc[self.gene_ids, 'mean'].values\n","        values = np.divide(row, mean_divide_by)\n","        # append <cls> token at the beginning\n","        genes = np.insert(self.gene_ids, 0, vocab[\"<cls>\"])\n","        values = np.insert(values, 0, config.pad_value, axis=1)\n","        genes = torch.from_numpy(genes).long()\n","        values = torch.from_numpy(values).float()\n","        output = {\n","            \"id\": idx,\n","            \"genes\": genes,\n","            \"expressions\": torch.nan_to_num(values, nan=0.0),\n","            \"batch_labels\": torch.from_numpy(self.batch_ids[idx]).long(),\n","            \"coordinates\": torch.from_numpy(self.coordinates[idx]),\n","        }\n","        return output"],"id":"b8b515a2c13dd7b7","outputs":[],"execution_count":8},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.593405Z","start_time":"2025-09-19T11:03:56.572855Z"},"id":"722d0983765f1ac6","executionInfo":{"status":"ok","timestamp":1758311449414,"user_tz":420,"elapsed":131,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["# Data collator. Adapted from scGPT-spatial.\n","from scgpt_spatial import binning\n","from dataclasses import dataclass, field\n","from typing import Callable, Dict, Mapping, Optional, Tuple\n","\n","import numpy as np\n","\n","\n","@dataclass\n","class DataCollator:\n","    \"\"\"\n","    Data collator for the mask value learning task. It pads the sequences to\n","    the maximum length in the batch and masks the gene expression values.\n","\n","    Args:\n","        do_padding (:obj:`bool`): whether to pad the sequences to the max length.\n","        pad_token_id (:obj:`int`, optional): the token id to use for padding.\n","            This is required if do_padding is True.\n","        pad_value (:obj:`int`): the value to use for padding the expression\n","            values to the max length.\n","        do_mlm (:obj:`bool`): whether to do masking with MLM.\n","        do_binning (:obj:`bool`): whether to bin the expression values.\n","        n_bins (:obj:`int`): the number of bins to use for binning.\n","        mlm_probability (:obj:`float`): the probability of masking with MLM.\n","        mask_value (:obj:`int`): the value to fill at the expression postions\n","            that are masked.\n","        max_length (:obj:`int`, optional): the maximum length of the sequences.\n","            This is required if do_padding is True.\n","        sampling (:obj:`bool`): whether to do sampling instead of truncation if\n","            length > max_length.\n","        reserve_keys (:obj:`List[str]`, optional): a list of keys in the examples\n","            to reserve in the output dictionary. Default to []. These fields\n","            will be kept unchanged in the output.\n","        append_tokens (:obj:`List[Callable]`, optional): a list of functions to\n","            append tokens to the beginning of the sequence. Each function takes\n","            an example as input and append tokens to the beginning of the sequence\n","            after `keep_first_n_tokens` tokens. This is useful when special tokens\n","            have been added to the beginning of the sequence. Default to [].\n","        keep_first_n_tokens (:obj:`int`): the number of tokens in the beginning\n","            of the sequence to keep unchanged from sampling. This is useful when\n","            special tokens have been added to the beginning of the sequence. **Note**\n","            that the `append_tokens` will be handled automatically, so only include\n","            other tokens in this number. Default to 1.\n","        data_style (:obj:`str`): the style of the data. If \"pcpt\", the data is\n","            masked and padded for perception training. If \"gen\", only the gene\n","            tokens are provided, but not the expression values, for pure generative\n","            training setting. If \"both\", the output will contain both fields above.\n","            Choices: \"pcpt\", \"gen\", \"both\". Default to \"pcpt\".\n","    \"\"\"\n","\n","    do_padding: bool = True\n","    pad_token_id: Optional[int] = None\n","    pad_value: int = 0\n","    do_mlm: bool = True\n","    do_binning: bool = True\n","    n_bins: int = 51\n","    mlm_probability: float = 0.15\n","    mask_value: int = -1\n","    max_length: Optional[int] = None\n","    sampling: bool = True\n","    reserve_keys: List[str] = field(default_factory=lambda: [])\n","    append_tokens: List[Callable] = field(default_factory=lambda: [])\n","    keep_first_n_tokens: int = 1  # for <cls>\n","    data_style: str = \"both\"\n","    device: str = \"cpu\"\n","\n","    def __post_init__(self):\n","        if self.do_padding:\n","            if self.pad_token_id is None:\n","                raise ValueError(\"`pad_token_id` is required if `do_padding`.\")\n","            if self.max_length is None:\n","                raise ValueError(\"`max_length` is required if `do_padding`.\")\n","\n","        if self.do_binning:\n","            if self.n_bins < 2:\n","                raise ValueError(\"`n_bins` must be greater than 1.\")\n","\n","        if isinstance(self.mlm_probability, float):\n","            if self.mlm_probability <= 0 or self.mlm_probability >= 1:\n","                raise ValueError(\"`mlm_probability` must be between 0 and 1.\")\n","        elif isinstance(self.mlm_probability, (list, tuple)):\n","            if min(self.mlm_probability) <= 0 or max(self.mlm_probability) >= 1:\n","                raise ValueError(\"`mlm_probability` must be between 0 and 1.\")\n","        else:\n","            raise ValueError(\n","                \"`mlm_probability` must be a float or iterable of floats.\")\n","\n","        if isinstance(self.reserve_keys, str):\n","            self.reserve_keys = [self.reserve_keys]\n","\n","        if len(self.append_tokens) > 0:\n","            self.original_prefix_n_tokens = self.keep_first_n_tokens\n","            self.keep_first_n_tokens = self.keep_first_n_tokens + len(\n","                self.append_tokens\n","            )\n","\n","        if self.keep_first_n_tokens < 0 or self.keep_first_n_tokens > self.max_length:\n","            raise ValueError(\n","                \"`keep_first_n_tokens` must be between 0 and `max_length` \"\n","                f\"({self.max_length}).\"\n","            )\n","\n","        if self.data_style not in [\"pcpt\", \"gen\", \"both\"]:\n","            raise ValueError(\n","                \"`data_style` must be one of 'pcpt', 'gen', 'both'.\")\n","\n","    def __call__(\n","            self, examples: List[Dict[str, torch.Tensor]]\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Args:\n","            examples (:obj:`List[Dict[str, torch.Tensor]]`): a list of data dicts.\n","                Each dict is for one cell. It contains multiple 1 dimensional tensors\n","                like the following exmaple:\n","                    {'id': tensor(184117),\n","                    'genes': tensor([36572, 17868, ..., 17072]),\n","                    'expressions': tensor([ 0.,  2., ..., 18.])}\n","\n","        Returns:\n","            :obj:`Dict[str, torch.Tensor]`: a dict of tensors.\n","        \"\"\"\n","        if len(self.reserve_keys) > 0:\n","            assert all(key in examples[0] for key in self.reserve_keys), (\n","                f\"reserve_keys must be a subset of the keys in the examples. \"\n","                f\"Got {self.reserve_keys} but expected keys in {list(examples[0].keys())}.\"\n","            )\n","\n","        if len(self.append_tokens) > 0:\n","            for i, func in enumerate(self.append_tokens):\n","                examples = self._append_token(\n","                    examples, func,\n","                    prefix_n_tokens=self.original_prefix_n_tokens + i\n","                )\n","\n","        if self.data_style == \"pcpt\":\n","            data_dict = self._call_pcpt(examples)\n","        elif self.data_style == \"gen\":\n","            data_dict = self._call_gen(examples)\n","        elif self.data_style == \"both\":\n","            data_dict = self._call_both(examples)\n","\n","        # add reserved keys\n","        for key in self.reserve_keys:\n","            data_ = [example[key] for example in examples]\n","            data_dict[key] = torch.stack(data_, dim=0).to(self.device)\n","\n","        return data_dict\n","\n","    def _append_token(\n","            self,\n","            examples: List[Dict[str, torch.Tensor]],\n","            func: Callable,\n","            prefix_n_tokens: int,\n","    ) -> Tuple[List[Dict[str, torch.Tensor]], int]:\n","        \"\"\"\n","        Append tokens to the beginning of the sequence. This is useful when special\n","        tokens have been added to the beginning of the sequence.\n","\n","        Args:\n","            examples (:obj:`List[Dict[str, torch.Tensor]]`): a list of data dicts.\n","                Each dict is for one cell. It contains multiple 1 dimensional tensors\n","                like the following exmaple:\n","                    {'id': tensor(184117),\n","                    'genes': tensor([36572, 17868, ..., 17072]),\n","                    'expressions': tensor([ 0.,  2., ..., 18.])}\n","            func (:obj:`Callable`): a function that takes an example as input and\n","                append tokens to the beginning of the sequence after `keep_first_n_tokens`\n","                tokens.\n","            prefix_n_tokens (:obj:`int`): the number of tokens in the beginning\n","                of the sequence to keep unchanged from sampling.\n","\n","        Returns:\n","            Tuple[List[Dict[str, torch.Tensor]], int]: the updated examples and\n","                keep_first_n_tokens.\n","        \"\"\"\n","        for i in range(len(examples)):\n","            examples[i] = func(examples[i], prefix_n_tokens)\n","        return examples\n","\n","    def _call_pcpt(\n","            self, examples: List[Dict[str, torch.Tensor]]\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Each example is like:\n","            {'id': tensor(184117),\n","            'genes': tensor([36572, 17868, ..., 17072]),\n","            'expressions': tensor([ 0.,  2., ..., 18.])}\n","\n","        Args:\n","            examples (:obj:`List[Dict[str, torch.Tensor]]`): a list of examples.\n","                Each example is a dictionary of tensors.\n","        Returns:\n","            :obj:`Dict[str, torch.Tensor]`: a dictionary of tensors.\n","        \"\"\"\n","        if not isinstance(examples[0], Mapping):\n","            return NotImplementedError\n","\n","        max_ori_len = max(len(example[\"genes\"]) for example in examples)\n","        _max_length = self.max_length if max_ori_len >= self.max_length else max_ori_len\n","\n","        # pad and truncate\n","        padded_genes = []\n","        padded_expressions = []\n","        for i in range(len(examples)):\n","            genes = examples[i][\"genes\"]\n","            expressions = examples[i][\"expressions\"]\n","            if self.do_binning:\n","                expressions[self.keep_first_n_tokens:] = binning(\n","                    row=expressions[self.keep_first_n_tokens:],\n","                    n_bins=self.n_bins,\n","                )\n","\n","            genes, expressions = self._sample_or_truncate_plus_pad(\n","                genes, expressions, _max_length\n","            )  # torch tensors of length _max_length\n","\n","            padded_genes.append(genes)\n","            padded_expressions.append(expressions)\n","\n","        padded_genes = torch.stack(padded_genes, dim=0).to(self.device)\n","        padded_expressions = torch.stack(padded_expressions, dim=0).to(\n","            self.device)\n","\n","        data_dict = {\n","            \"gene\": padded_genes,\n","            \"expr\": padded_expressions,\n","        }\n","\n","        # mask\n","        if self.do_mlm:\n","            masked_expressions = self._mask(\n","                padded_expressions, self.keep_first_n_tokens\n","            )\n","        else:\n","            masked_expressions = padded_expressions\n","        data_dict[\"masked_expr\"] = masked_expressions\n","\n","        return data_dict\n","\n","    def _call_gen(\n","            self,\n","            examples: List[Dict[str, torch.Tensor]]\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        This method will simply return the gene ids, with needed padding. There is\n","        no masking for pure generative training, and no input of expr values.\n","\n","        Each example is like:\n","            {'id': tensor(184117),\n","            'genes': tensor([36572, 17868, ..., 17072])}\n","\n","        Returns:\n","            Dict[str, torch.Tensor]: a dict of tensors.\n","            Example:\n","                {'pcpt_gene': tensor([[36572, 17868, ..., 17072],\n","                                        [36572, 17868, ..., 17072],\n","                                        ...,\n","                                        [36572, 17868, ..., 17072]]),\n","                'pcpt_expr': tensor([[ 0.,  2., ..., 18.],\n","                                        [ 0.,  2., ..., 18.],\n","                                        ...,\n","                                        [ 0.,  2., ..., 18.]])}\n","        \"\"\"\n","        raise NotImplementedError(\"Code coming soon with pre-training branch.\")\n","\n","    def _call_both(\n","            self,\n","            examples: List[Dict[str, torch.Tensor]],\n","    ) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        This method will split the input into the peception part and the generation\n","        part. The perception part will be processed into gene ids and expr values,\n","        and the generation part will be processed into gene ids only.\n","\n","        By default, the mlm_probability will be used to select the genese assigned to\n","        the generation part.\n","\n","        Each example is like:\n","            {'id': tensor(184117),\n","            'genes': tensor([36572, 17868, ..., 17072]),\n","            'expressions': tensor([ 0.,  2., ..., 18.])}\n","\n","        Args:\n","            gen_prob (float, optional): the probability of a gene being assigned to\n","                the generation part. If not provided, the mlm_probability will be used.\n","\n","        Returns:\n","            Dict[str, torch.Tensor]: a dict of tensors.\n","            Example:\n","                {'pcpt_gene': tensor([[36572, 17868, ..., 17072],\n","                                        [36572, 17868, ..., 17072],\n","                                        ...,\n","                                        [36572, 17868, ..., 17072]]),\n","                'pcpt_expr': tensor([[ 0.,  2., ..., 18.],\n","                                        [ 0.,  2., ..., 18.],\n","                                        ...,\n","                                        [ 0.,  2., ..., 18.]]),\n","                'gen_gene': tensor([[36573, 17869, ..., 17073],\n","                                        [36573, 17869, ..., 17073],\n","                                        ...,\n","                                        [36573, 17869, ..., 17073]]),\n","                'gen_expr_target': tensor([[ 1.,  3., ..., 19.],\n","                                        [ 1.,  3., ..., 19.],\n","                                        ...,\n","                                        [ 1.,  3., ..., 19.]])}\n","        \"\"\"\n","        if not isinstance(examples[0], Mapping):\n","            return NotImplementedError\n","\n","        max_ori_len = max(len(example[\"genes\"]) for example in examples)\n","        _max_length = self.max_length if max_ori_len >= self.max_length else max_ori_len\n","\n","        # pad and truncate\n","        padded_genes = []\n","        padded_expressions = []\n","        gen_genes, gen_expr_targets = [], []\n","        for i in range(len(examples)):\n","            for expression in examples[i][\"expressions\"]:\n","                genes = examples[i][\"genes\"]\n","                # randomly choose query genes with around 1:1 zero and non-zero expressions\n","                nz_idx = expression.nonzero().squeeze()\n","                z_idx = (expression == 0).nonzero().squeeze()\n","\n","                total_gen_gene_num = int(_max_length * self.mlm_probability)\n","                if nz_idx.shape[0] <= z_idx.shape[0]:\n","                    nz_gen_gene_num = int(\n","                        min(nz_idx.shape[0] * 0.9, total_gen_gene_num // 2))\n","                    z_gen_gene_num = total_gen_gene_num - nz_gen_gene_num\n","                else:\n","                    z_gen_gene_num = int(\n","                        min(z_idx.shape[0] * 0.9, total_gen_gene_num // 2))\n","                    nz_gen_gene_num = total_gen_gene_num - z_gen_gene_num\n","                z_gen_idx = z_idx[\n","                    torch.randperm(z_idx.shape[0])[:z_gen_gene_num]]\n","                nz_gen_idx = nz_idx[\n","                    torch.randperm(nz_idx.shape[0])[:nz_gen_gene_num]]\n","                gen_gene_idx = torch.cat((nz_gen_idx, z_gen_idx), dim=0)\n","                gen_gene, gen_expr_target = self._sample_or_truncate_plus_pad(\n","                    genes[gen_gene_idx], expression[gen_gene_idx], _max_length\n","                )\n","                gen_genes.append(gen_gene)\n","                gen_expr_targets.append(gen_expr_target)\n","\n","                # choose pcpt genes\n","                mask = torch.ones(expression.shape[0], dtype=torch.bool)\n","                mask[gen_gene_idx] = False\n","                expression = expression[mask]\n","                genes = genes[mask]\n","                if self.do_binning:\n","                    expression[self.keep_first_n_tokens:] = binning(\n","                        row=expression[self.keep_first_n_tokens:],\n","                        n_bins=self.n_bins,\n","                    )\n","\n","                genes, expression = self._sample_or_truncate_plus_pad(\n","                    genes, expression, _max_length\n","                )  # torch tensors of length _max_length\n","\n","                padded_genes.append(genes)\n","                padded_expressions.append(expression)\n","\n","        padded_gene_genes = torch.stack(gen_genes, dim=0).to(self.device)\n","        padded_gen_expr_target = torch.stack(gen_expr_targets, dim=0).to(\n","            self.device)\n","        padded_genes = torch.stack(padded_genes, dim=0).to(self.device)\n","        padded_expressions = torch.stack(padded_expressions, dim=0).to(\n","            self.device)\n","\n","        data_dict = {\n","            \"gen_gene\": padded_gene_genes,\n","            \"gen_expr_target\": padded_gen_expr_target,\n","            \"pcpt_gene\": padded_genes,\n","            \"pcpt_expr\": padded_expressions,\n","        }\n","\n","        return data_dict\n","\n","    def get_mlm_probability(self) -> float:\n","        \"\"\"\n","        Get the mlm probability for the current step.\n","        \"\"\"\n","        if isinstance(self.mlm_probability, float):\n","            return self.mlm_probability\n","        elif isinstance(self.mlm_probability, list):\n","            # random choose a probability\n","            return np.random.choice(self.mlm_probability)\n","        else:\n","            raise ValueError(\n","                \"mlm_probability must be a float or a list of floats, \"\n","                f\"but got {self.mlm_probability}.\"\n","            )\n","\n","    def _mask(\n","            self, expressions: torch.Tensor, keep_first_n_tokens: int = 0\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Mask the expression values with MLM.\n","        \"\"\"\n","        if keep_first_n_tokens > 0:\n","            result_ = self._mask(\n","                expressions[:, keep_first_n_tokens:],\n","                keep_first_n_tokens=0,\n","            )\n","            return torch.cat([expressions[:, :keep_first_n_tokens], result_],\n","                             dim=1)\n","\n","        shape = expressions.shape\n","\n","        probability_matrix = torch.full(shape, self.get_mlm_probability())\n","        # set padded postion probability to 0\n","        probability_matrix[expressions.eq(self.pad_value)] = 0\n","        if self.keep_first_n_tokens > 0:\n","            probability_matrix[:, : self.keep_first_n_tokens] = 0\n","\n","        mask = torch.bernoulli(probability_matrix).bool()\n","        mask = mask.to(self.device)\n","\n","        masked_expressions = expressions.masked_fill(mask, self.mask_value)\n","        return masked_expressions\n","\n","    def _sample_or_truncate_plus_pad(\n","            self,\n","            genes: torch.LongTensor,\n","            expressions: torch.Tensor,\n","            max_length: int,\n","    ) -> Tuple[torch.LongTensor, torch.Tensor]:\n","        assert len(genes) == len(expressions)\n","        if len(genes) == max_length:\n","            return genes, expressions\n","        if len(genes) > max_length:  # sample or truncate\n","            if self.sampling:\n","                return self._sample(genes, expressions, max_length)\n","            else:\n","                return genes[:max_length], expressions[:max_length]\n","        else:  # pad\n","            return self._pad(genes, expressions, max_length)\n","\n","    def _sample(\n","            self,\n","            genes: torch.LongTensor,\n","            expressions: torch.Tensor,\n","            max_length: int,\n","    ) -> Tuple[torch.LongTensor, torch.Tensor]:\n","        # NOTE: the fastest way to sample in torch has been benchmarked here\n","        # https://discuss.pytorch.org/t/torch-equivalent-of-numpy-random-choice/16146/19\n","        # it shows the randperm on gpu is the fastest.\n","        # NOTE: also, the current implementation permute the orders of the genes\n","        # and expressions, although it is probably a nice argmentation.\n","        if self.keep_first_n_tokens == 0:\n","            indices = torch.randperm(len(genes), device=self.device)[\n","                      :max_length]\n","            return genes[indices], expressions[indices]\n","\n","        # keep the first n tokens unchanged\n","        _n = self.keep_first_n_tokens\n","        indices = torch.randperm(len(genes) - _n, device=self.device)[\n","                  : max_length - _n]\n","        indices = torch.cat([torch.arange(_n), indices + _n], dim=0)\n","        return genes[indices], expressions[indices]\n","\n","    def _pad(\n","            self,\n","            genes: torch.LongTensor,\n","            expressions: torch.Tensor,\n","            max_length: int,\n","    ):\n","        genes = torch.cat(\n","            [\n","                genes,\n","                torch.full(\n","                    (max_length - len(genes),),\n","                    self.pad_token_id,\n","                    dtype=genes.dtype,\n","                    device=self.device,\n","                ),\n","            ]\n","        )\n","        expressions = torch.cat(\n","            [\n","                expressions,\n","                torch.full(\n","                    (max_length - len(expressions),),\n","                    self.pad_value,\n","                    dtype=expressions.dtype,\n","                    device=self.device,\n","                ),\n","            ]\n","        )\n","        return genes, expressions"],"id":"722d0983765f1ac6","outputs":[],"execution_count":9},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.604964Z","start_time":"2025-09-19T11:03:56.600745Z"},"id":"7d7f76ac7ce028fe","executionInfo":{"status":"ok","timestamp":1758311449432,"user_tz":420,"elapsed":17,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["# spatial aware sampler\n","from __future__ import annotations\n","import random\n","from typing import List\n","from typing import Iterable\n","\n","import torch\n","from torch.utils.data import Sampler\n","\n","\n","# per slide (default single slide)\n","class KNNPatchBatchSampler(Sampler[List[int]]):\n","    def __init__(\n","            self,\n","            coordinates,\n","            patches_per_batch: int = 8,\n","            # batch_size = (k_neighbors + 1) * patches_per_batch\n","            k_neighbors: int = 7,  # neighbors per patch\n","            seed: int = 42,\n","    ):\n","        self.rng = random.Random(seed)\n","        self.patch_num = len(coordinates) // (k_neighbors + 1)\n","        self.patches_per_batch = patches_per_batch\n","        self.k_neighbors = k_neighbors\n","        # one anchor per patch\n","        anchor_indices = random.sample(range(len(coordinates)), self.patch_num)\n","\n","        # pre-compute knn\n","        coordinates = torch.tensor(coordinates, dtype=torch.float32)\n","        dist = torch.cdist(coordinates.float(), coordinates.float(),\n","                           p=2)  # Shape [n_cells, n_cells]\n","        self.patches = []\n","        for anchor_index in anchor_indices:\n","            topk_indices = \\\n","                torch.topk(dist, k=k_neighbors + 1, dim=-1, largest=False,\n","                           sorted=True)[\n","                    1]  # Shape [n_cells, (k_neighbors + 1)]\n","            if torch.isinf(topk_indices[anchor_index]).any().item():\n","                # DropLast = true\n","                break\n","            self.patches.append(topk_indices[anchor_index])\n","            dist[anchor_index, :] = torch.inf\n","            dist[:, anchor_index] = torch.inf\n","\n","    def __iter__(self) -> Iterable[List[int]]:\n","        random_patch_idx = torch.randperm(self.patch_num)\n","        for i in range(0, self.patch_num, self.patches_per_batch):\n","            batch = [x for j in random_patch_idx[i:i + self.patches_per_batch]\n","                     for x in self.patches[j]]\n","            self.rng.shuffle(batch)\n","            yield batch\n","\n","    def __len__(self) -> int:\n","        return self.patch_num * (self.k_neighbors + 1)"],"id":"7d7f76ac7ce028fe","outputs":[],"execution_count":10},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.614238Z","start_time":"2025-09-19T11:03:56.611478Z"},"id":"2e01e0827b33d7cf","executionInfo":{"status":"ok","timestamp":1758311449436,"user_tz":420,"elapsed":2,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["# prepare data loader\n","def prepare_dataloader(count_matrix, gene_ids, batch_labels, coordinates, vocab,\n","                       config: Config) -> DataLoader:\n","    dataset = Dataset(count_matrix, gene_ids, batch_labels, coordinates, config)\n","\n","    collator = DataCollator(\n","        do_padding=True,\n","        pad_token_id=vocab[config.pad_token],\n","        pad_value=config.pad_value,\n","        do_mlm=True,\n","        do_binning=True,\n","        n_bins=51,\n","        mlm_probability=config.mask_ratio,\n","        mask_value=config.mask_value,\n","        max_length=config.max_seq_len,\n","        sampling=True,\n","        reserve_keys=['coordinates', 'batch_labels'],\n","        keep_first_n_tokens=1,\n","        data_style='both',\n","    )\n","    return DataLoader(\n","        dataset,\n","        sampler=KNNPatchBatchSampler(coordinates,\n","                                     patches_per_batch=config.batch_size // (\n","                                             config.n_neighbors + 1),\n","                                     k_neighbors=config.n_neighbors,\n","                                     seed=config.seed),\n","        collate_fn=collator,\n","        num_workers=0,\n","        pin_memory=True,\n","    )"],"id":"2e01e0827b33d7cf","outputs":[],"execution_count":11},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:20:48.277651Z","start_time":"2025-09-19T11:20:48.249846Z"},"id":"2117887534505816","executionInfo":{"status":"ok","timestamp":1758311449481,"user_tz":420,"elapsed":44,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["from torch import nn\n","\n","\n","def train_epoch(epoch, model: nn.Module, loader, scheduler, optimizer,\n","                criterion, scaler, device, vocab, config: config):\n","    \"\"\"\n","    Train the model for one epoch.\n","    \"\"\"\n","\n","    total_loss, total_mse, total_geps_intra, total_geps_inter = 0.0, 0.0, 0.0, 0.0\n","\n","    num_batches = len(loader) // config.batch_size\n","    start_time = time.time()\n","    for batch, batch_data in enumerate(loader):\n","        pcpt_gene = batch_data[\"pcpt_gene\"].to(device)\n","        pcpt_expr = batch_data[\"pcpt_expr\"].to(device)\n","        pcpt_key_padding_mask = pcpt_gene.eq(vocab[config.pad_token])\n","        gen_gene = batch_data[\"gen_gene\"].to(device)\n","        gen_expr_target = batch_data[\"gen_expr_target\"].to(device)\n","        gen_key_padding_mask = gen_gene.eq(vocab[config.pad_token])\n","        coordinates = batch_data[\"coordinates\"].to(device)\n","        batch_labels = batch_data[\"batch_labels\"].to(device)\n","\n","        with (torch.cuda.amp.autocast(enabled=False)):\n","            output_dict = model(\n","                pcpt_genes=pcpt_gene,\n","                pcpt_values=pcpt_expr,\n","                pcpt_key_padding_mask=pcpt_key_padding_mask,\n","                gen_genes=gen_gene,\n","                gen_key_padding_mask=gen_key_padding_mask,\n","                batch_labels=batch_labels,\n","                coordinates=coordinates,\n","                CLS=False,\n","                MVC=True,\n","                ECS=False,\n","                MVC_impute=True,\n","                do_sample=False,\n","                input_cell_emb=None,\n","                generative_training=True\n","            )  # dict: key: (batch, embsize)\n","\n","            # gepc intra\n","            masked_positions = gen_gene.ne(vocab[config.pad_token])\n","            loss = loss_mse = criterion(\n","                output_dict[\"gen_preds\"], gen_expr_target, masked_positions\n","            )\n","            metrics_to_log = {\"train/mse\": loss_mse.item()}\n","\n","            geps_target = torch.cat([pcpt_expr, gen_expr_target], dim=1)\n","            pcpt_mask = torch.zeros(pcpt_expr.shape, dtype=torch.bool).to(\n","                device)\n","            geps_masked_position = torch.cat(\n","                [pcpt_mask, masked_positions], dim=1)\n","            loss_geps_intra = criterion(\n","                output_dict[\"mvc_output\"], geps_target, geps_masked_position\n","            )\n","            loss += loss_geps_intra\n","            metrics_to_log.update(\n","                {\"train/loss_geps_intra\": loss_geps_intra.item()})\n","\n","            # gepc inter\n","            loss_geps_inter = criterion(\n","                output_dict[\"impute_pred\"], geps_target, geps_masked_position\n","            )\n","            loss = loss + loss_geps_inter\n","            metrics_to_log.update(\n","                {\"train/loss_geps_inter\": loss_geps_inter.item()})\n","\n","        model.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        with warnings.catch_warnings(record=True) as w:\n","            warnings.filterwarnings(\"always\")\n","            total_norm = torch.nn.utils.clip_grad_norm_(\n","                model.parameters(),\n","                1.0,\n","                error_if_nonfinite=False\n","            )\n","            if not torch.isfinite(total_norm):\n","              print(f\"[Warn] grad total_norm is {total_norm} at step {batch} (clipped).\")\n","\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # logging\n","        total_loss += loss.item()\n","        total_mse += loss_mse.item()\n","        total_geps_intra += loss_geps_intra.item()\n","        total_geps_inter += loss_geps_inter.item()\n","        logging_interval = 100\n","        if batch % logging_interval == 0 and batch > 0:\n","            lr = scheduler.get_last_lr()[0]\n","            ms_per_batch = (time.time() - start_time) * logging_interval\n","            cur_loss = total_loss / logging_interval\n","            cur_mse = total_mse / logging_interval\n","            cur_gepc_intra = total_geps_intra / logging_interval\n","            scgpt_spatial.logger.info(\n","                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n","                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n","                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} |\"\n","                + (f\"gepc {cur_gepc_intra:5.2f} \")\n","            )\n","            total_loss = 0\n","            total_mse = 0\n","            total_geps_intra = 0\n","            total_geps_inter = 0\n","            start_time = time.time()"],"id":"2117887534505816","outputs":[],"execution_count":12},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.640577Z","start_time":"2025-09-19T11:03:56.637023Z"},"id":"f337599450e00e07","executionInfo":{"status":"ok","timestamp":1758311449483,"user_tz":420,"elapsed":1,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["from scgpt.loss import masked_relative_error\n","\n","def evaluate(model: nn.Module, loader: DataLoader, criterion, device, vocab,\n","             config: config):\n","    \"\"\"\n","    Evaluate the model on the evaluation data.\n","    \"\"\"\n","    model.eval()\n","    total_loss = 0.0\n","    total_error = 0.0\n","    total_num = 0\n","    with torch.no_grad():\n","        for batch_data in loader:\n","            pcpt_gene = batch_data[\"pcpt_gene\"].to(device)\n","            pcpt_expr = batch_data[\"pcpt_expr\"].to(device)\n","            pcpt_key_padding_mask = pcpt_gene.eq(vocab[config.pad_token])\n","            gen_gene = batch_data[\"gen_gene\"].to(device)\n","            gen_expr_target = batch_data[\"gen_expr_target\"].to(device)\n","            gen_key_padding_mask = gen_gene.eq(vocab[config.pad_token])\n","            coordinates = batch_data[\"coordinates\"].to(device)\n","            batch_labels = batch_data[\"batch_labels\"].to(device)\n","\n","            with torch.cuda.amp.autocast(enabled=False):\n","                output_dict = model(\n","                    pcpt_genes=pcpt_gene,\n","                    pcpt_values=pcpt_expr,\n","                    pcpt_key_padding_mask=pcpt_key_padding_mask,\n","                    gen_genes=gen_gene,\n","                    gen_key_padding_mask=gen_key_padding_mask,\n","                    batch_labels=batch_labels,\n","                    coordinates=coordinates,\n","                    CLS=False,\n","                    MVC=True,\n","                    ECS=False,\n","                    MVC_impute=True,\n","                    do_sample=False,\n","                    input_cell_emb=None,\n","                    generative_training=True\n","                )  # dict: key: (batch, embsize)\n","\n","            masked_positions = gen_gene.ne(vocab[config.pad_token])\n","            loss = criterion(\n","                output_dict[\"gen_preds\"], gen_expr_target, masked_positions\n","            )\n","            mre = masked_relative_error(\n","                output_dict[\"gen_preds\"], gen_expr_target, masked_positions\n","            )\n","            total_error += mre.item()\n","\n","            # gepc intra\n","            geps_target = torch.cat([pcpt_expr, gen_expr_target], dim=1)\n","            pcpt_mask = torch.zeros(pcpt_expr.shape, dtype=torch.bool).to(\n","                device)\n","            geps_masked_position = torch.cat([pcpt_mask, masked_positions], dim=1)\n","            loss_geps_intra = criterion(\n","                output_dict[\"mvc_output\"], geps_target, geps_masked_position\n","            )\n","            loss += loss_geps_intra\n","\n","            # gepc inter\n","            loss_geps_inter = criterion(\n","                output_dict[\"impute_pred\"], geps_target, geps_masked_position\n","            )\n","            loss = loss + loss_geps_inter\n","\n","            total_loss += loss.item()\n","            total_num += len(pcpt_gene)\n","\n","    return total_loss / total_num, total_error / total_num"],"id":"f337599450e00e07","outputs":[],"execution_count":13},{"metadata":{"id":"5800afd330b8fc88"},"cell_type":"markdown","source":["## Unsupervised Finetune"],"id":"5800afd330b8fc88"},{"metadata":{"ExecuteTime":{"end_time":"2025-09-19T11:03:56.648792Z","start_time":"2025-09-19T11:03:56.646943Z"},"id":"40f2500cf4affa2","executionInfo":{"status":"ok","timestamp":1758311449497,"user_tz":420,"elapsed":10,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["# set up configs\n","config = Config(h5ad='data/1_visium.h5ad',\n","                ckpt_dir='checkpoints/scGPT_spatial_v1',\n","                finetuned_ckpt_dir='finetuned_checkpoints/0919_epoches_5',\n","                batch_key_col='protocol',\n","                gene_names_col='gene_name',\n","                coordinates_x_col='array_row',\n","                coordinates_y_col='array_col',\n","                rep_key='X_scGPT_finetuned',\n","                epochs=5,\n","                batch_size=16,\n","                n_neighbors=7)"],"id":"40f2500cf4affa2","outputs":[],"execution_count":14},{"metadata":{"jupyter":{"is_executing":true},"ExecuteTime":{"start_time":"2025-09-19T11:20:50.664569Z"},"id":"8a2efb4ebcdc3e01","outputId":"8da5ec57-2e2c-44c3-df54-a89ebbefedf3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758315101475,"user_tz":420,"elapsed":3651977,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","from scgpt.loss import masked_mse_loss\n","import copy\n","import time\n","import logging\n","\n","\"\"\"\n","Fine tune scGPT-spatial FM and save checkpoints.\n","\"\"\"\n","torch.manual_seed(config.seed)\n","np.random.seed(config.seed)\n","scgpt_spatial.logger.setLevel(logging.INFO)\n","\n","finetuned_ckpt_dir = Path(config.finetuned_ckpt_dir)\n","finetuned_ckpt_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Step 1: load pre-trained model and vocab\n","model, vocab, _ = load_backbone_and_vocab(Path(config.ckpt_dir), config.device)\n","\n","# Step 2: load and split data. Will do binning in Dataset.\n","gene_ids, train_data, valid_data, train_batch_labels, valid_batch_labels, train_xy, valid_xy = split_dataset(\n","    Path(config.h5ad), config, vocab)\n","\n","# Step 3: finetune scGPT-spatial with MVC and impute MVC\n","criterion = masked_mse_loss\n","optimizer = torch.optim.Adam(\n","    model.parameters(), lr=config.lr, eps=1e-8\n",")\n","gamma = config.schedule_ratio\n","if isinstance(gamma, (tuple, list)):\n","    gamma = float(gamma[0])\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=gamma)\n","scaler = torch.cuda.amp.GradScaler(enabled=False)\n","\n","best_val_loss = float(\"inf\")\n","best_model = None\n","for epoch in range(1, config.epochs + 1):\n","    epoch_start_time = time.time()\n","    train_loader = prepare_dataloader(\n","        count_matrix=train_data,\n","        gene_ids=gene_ids,\n","        batch_labels=train_batch_labels,\n","        coordinates=train_xy,\n","        vocab=vocab,\n","        config=config,\n","    )\n","    valid_loader = prepare_dataloader(\n","        count_matrix=valid_data,\n","        gene_ids=gene_ids,\n","        batch_labels=valid_batch_labels,\n","        coordinates=valid_xy,\n","        vocab=vocab,\n","        config=config,\n","    )\n","\n","    train_epoch(\n","        epoch,\n","        model,\n","        loader=train_loader,\n","        scheduler=scheduler,\n","        optimizer=optimizer,\n","        scaler=scaler,\n","        criterion=criterion,\n","        device=config.device,\n","        vocab=vocab,\n","        config=config\n","    )\n","    val_loss, val_mre = evaluate(\n","        model,\n","        loader=valid_loader,\n","        criterion=criterion,\n","        device=config.device,\n","        vocab=vocab,\n","        config=config\n","    )\n","    elapsed = time.time() - epoch_start_time\n","    scgpt_spatial.logger.info(\"-\" * 89)\n","    scgpt_spatial.logger.info(\n","        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n","        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n","    )\n","    scgpt_spatial.logger.info(\"-\" * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","    best_model = copy.deepcopy(model)\n","    best_model_epoch = epoch\n","    scgpt_spatial.logger.info(\n","        f\"Best model with score {best_val_loss:5.4f}\")\n","\n","    scheduler.step()\n","\n","scgpt_spatial.logger.info(f\"Saving model to {config.finetuned_ckpt_dir}\")\n","torch.save(best_model.state_dict(),\n","            Path(config.finetuned_ckpt_dir, \"best_model.pt\"))\n"],"id":"8a2efb4ebcdc3e01","outputs":[{"output_type":"stream","name":"stderr","text":["INFO:scGPT-spatial:Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])\n","INFO:scGPT-spatial:Loading parameter encoder.enc_norm.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter encoder.enc_norm.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter flag_encoder.weight with shape torch.Size([2, 512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.norm.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.norm.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter batch_encoder.embedding.weight with shape torch.Size([4, 512])\n","INFO:scGPT-spatial:Loading parameter batch_encoder.enc_norm.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter batch_encoder.enc_norm.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.0.weight with shape torch.Size([512, 1024])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.0.weight with shape torch.Size([512, 1024])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.0.weight with shape torch.Size([512, 1024])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.0.weight with shape torch.Size([512, 1024])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.gate.gate.weight with shape torch.Size([4, 1024])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.gate.gate.bias with shape torch.Size([4])\n","INFO:scGPT-spatial:Loading parameter mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter mvc_decoder.gene2query.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter mvc_decoder.W.weight with shape torch.Size([1024, 512])\n","INFO:scGPT-spatial:Loading parameter impute_mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter impute_mvc_decoder.gene2query.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter impute_mvc_decoder.W.weight with shape torch.Size([1024, 512])\n","INFO:scGPT-spatial:Train set number of samples: 3798  Valid set number of samples: 423\n","INFO:scGPT-spatial:| epoch   1 | 100/237 batches | lr 0.0001 | ms/batch 28491.08 | loss  9.89 | mse  5.79 |gepc  4.47 \n","INFO:scGPT-spatial:| epoch   1 | 200/237 batches | lr 0.0001 | ms/batch 28717.46 | loss  1.14 | mse  0.67 |gepc  0.40 \n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:| end of epoch   1 | time: 720.02s | valid loss/mse 0.0564 | mre 8654.9350\n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:Best model with score 0.0564\n","INFO:scGPT-spatial:| epoch   2 | 100/237 batches | lr 0.0000 | ms/batch 29051.29 | loss  0.85 | mse  0.55 |gepc  0.30 \n","INFO:scGPT-spatial:| epoch   2 | 200/237 batches | lr 0.0000 | ms/batch 28741.51 | loss  0.77 | mse  0.50 |gepc  0.27 \n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:| end of epoch   2 | time: 725.50s | valid loss/mse 0.0440 | mre 8134.9395\n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:Best model with score 0.0440\n","INFO:scGPT-spatial:| epoch   3 | 100/237 batches | lr 0.0000 | ms/batch 29048.87 | loss  0.84 | mse  0.51 |gepc  0.26 \n","INFO:scGPT-spatial:| epoch   3 | 200/237 batches | lr 0.0000 | ms/batch 28730.71 | loss  0.78 | mse  0.49 |gepc  0.27 \n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:| end of epoch   3 | time: 725.95s | valid loss/mse 0.0443 | mre 8196.2536\n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:Best model with score 0.0440\n","INFO:scGPT-spatial:| epoch   4 | 100/237 batches | lr 0.0000 | ms/batch 29031.11 | loss  0.75 | mse  0.46 |gepc  0.24 \n","INFO:scGPT-spatial:| epoch   4 | 200/237 batches | lr 0.0000 | ms/batch 28729.64 | loss  0.81 | mse  0.53 |gepc  0.29 \n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:| end of epoch   4 | time: 724.82s | valid loss/mse 0.0469 | mre 8388.9830\n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:Best model with score 0.0440\n","INFO:scGPT-spatial:| epoch   5 | 100/237 batches | lr 0.0000 | ms/batch 29032.81 | loss  0.74 | mse  0.46 |gepc  0.24 \n","INFO:scGPT-spatial:| epoch   5 | 200/237 batches | lr 0.0000 | ms/batch 28718.03 | loss  0.69 | mse  0.45 |gepc  0.24 \n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:| end of epoch   5 | time: 725.33s | valid loss/mse 0.0423 | mre 8286.4687\n","INFO:scGPT-spatial:-----------------------------------------------------------------------------------------\n","INFO:scGPT-spatial:Best model with score 0.0423\n","INFO:scGPT-spatial:Saving model to finetuned_checkpoints/0919_epoches_5\n"]}],"execution_count":15},{"cell_type":"markdown","source":["## Example: Zero-shot Domain Detection w/ Finefuned Model"],"metadata":{"id":"lvC1qxB5zyWh"},"id":"lvC1qxB5zyWh"},{"cell_type":"code","source":["# load data & simple pre process\n","import numpy as np\n","import pandas as pd\n","import scanpy as sc\n","\n","\n","adata = sc.read_h5ad('data/1_visium.h5ad')\n","adata = adata[np.logical_not(adata.obs['ground_truth'].isna())]  #remove NAN\n","adata.var['gene_name'] = adata.var.index\n","\n","adata.obs['protocol'] = 'visium'\n","PROTO2ID = {\n","  \"merfish\": 0,\n","  \"visium\": 1,\n","  \"visium_hd\": 2,\n","  \"xenium\": 3\n","}   # keep aligned with scGPT-spatial v1 checkpoints\n","adata.obs['protocol'] = adata.obs['protocol'].map(PROTO2ID).astype(int).to_numpy()\n","\n","print(adata)\n","print(adata.obs.ground_truth.unique())\n","adata.write('data/1_visium.h5ad')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLbykiThz5bM","executionInfo":{"status":"ok","timestamp":1758315102478,"user_tz":420,"elapsed":987,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}},"outputId":"defc0013-1fe6-466a-9b1f-95d31d885881"},"id":"oLbykiThz5bM","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["AnnData object with n_obs  n_vars = 4221  33538\n","    obs: 'in_tissue', 'array_row', 'array_col', 'Region', 'ground_truth', 'protocol'\n","    var: 'gene_ids', 'feature_types', 'genome', 'gene_name'\n","    uns: 'spatial'\n","    obsm: 'spatial'\n","['Layer1', 'Layer3', 'WM', 'Layer6', 'Layer5', 'Layer2', 'Layer4']\n","Categories (7, object): ['Layer1', 'Layer2', 'Layer3', 'Layer4', 'Layer5', 'Layer6', 'WM']\n"]}]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","import scgpt_spatial\n","\n","# run scGPT-spatial inference\n","model_dir = 'checkpoints/scGPT_spatial_v1'\n","finetuned_model_dir = 'finetuned_checkpoints/0919_epoches_5'\n","adata = scgpt_spatial.tasks.embed_data(\n","    adata,\n","    finetuned_model_dir,\n","    gene_col='gene_name',\n","    obs_to_save=['array_row', 'array_col', 'ground_truth'],\n","    batch_size=64,\n","    return_new_adata=False,\n","    use_fast_transformer=False\n",")\n","print(adata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_MB9Qfoz6Yu","executionInfo":{"status":"ok","timestamp":1758315154225,"user_tz":420,"elapsed":51743,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}},"outputId":"62571ee3-7c9a-41f3-fd25-e4cc567a9f0c"},"id":"s_MB9Qfoz6Yu","execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:scGPT-spatial:match 23325/33538 genes in vocabulary of size 60697.\n","INFO:scGPT-spatial:Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])\n","INFO:scGPT-spatial:Loading parameter encoder.enc_norm.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter encoder.enc_norm.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter flag_encoder.weight with shape torch.Size([2, 512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.norm.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter value_encoder.norm.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.0.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.1.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.2.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.0.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.2.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.2.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.4.weight with shape torch.Size([1, 512])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.experts.3.fc.4.bias with shape torch.Size([1])\n","INFO:scGPT-spatial:Loading parameter decoder.moe.gate.gate.bias with shape torch.Size([4])\n","INFO:scGPT-spatial:Loading parameter mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter mvc_decoder.gene2query.bias with shape torch.Size([512])\n","INFO:scGPT-spatial:Loading parameter impute_mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n","INFO:scGPT-spatial:Loading parameter impute_mvc_decoder.gene2query.bias with shape torch.Size([512])\n","Embedding cells: 100%|| 66/66 [00:45<00:00,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":["AnnData object with n_obs  n_vars = 4221  23325\n","    obs: 'in_tissue', 'array_row', 'array_col', 'Region', 'ground_truth', 'protocol'\n","    var: 'gene_ids', 'feature_types', 'genome', 'gene_name', 'id_in_vocab'\n","    uns: 'spatial'\n","    obsm: 'spatial', 'X_scGPT'\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import scanpy as sc\n","from sklearn.metrics import silhouette_score\n","\n","def predict_domain_using_embedding(h5ad: str,\n","                                   dom_key: str = \"domain\",  # output label for clustering result\n","                                   method: str = \"leiden\", # \"leiden\" or \"louvain\"\n","                                   rep_key: str = \"X_scGPT\",  # adata.obsm[rep_key]: embedding\n","                                   target_clusters: int = 6,\n","                                   n_neighbors: int = 15,\n","                                   resolution_grid: Iterable[float] = (0.3, 0.5,\n","                                                                       0.8, 1.0,\n","                                                                       1.2),\n","                                   return_silhouette: bool = True,\n","                                   ):\n","    \"\"\"\n","    predict domain using generated embedding.\n","    Returns:\n","      labels: np.ndarray[int]  Domain labels for each spot (encoded integers)\n","      best_res: Optional[float]  Resolution to use (if automatically selected)\n","      best_score: Optional[float]  Silhouette score (if calculated)\n","    Side Effects:\n","      - Writes adata.obsm[rep_key] = (n_spot, D)\n","      - Writes adata.obs[dom_key] = pandas.Categorical\n","    Depends:\n","      - adata.obsm[rep_key]\n","    \"\"\"\n","    sc.pp.neighbors(adata, use_rep=rep_key, n_neighbors=n_neighbors)\n","\n","    def _cluster_at(res):\n","        if method == \"leiden\":\n","            sc.tl.leiden(adata, resolution=res, key_added=f\"{dom_key}_tmp\")\n","        elif method == \"louvain\":\n","            sc.tl.louvain(adata, resolution=res, key_added=f\"{dom_key}_tmp\")\n","        else:\n","            raise ValueError(\"method must be 'leiden' or 'louvain'\")\n","        return adata.obs[f\"{dom_key}_tmp\"].astype(\n","            \"category\").cat.codes.to_numpy()\n","\n","    best_res, best_score, best_labels, best_clusters = None, -1.0, None, None\n","    if target_clusters is None:\n","        for res in resolution_grid:\n","            labels = _cluster_at(res)\n","            if len(np.unique(labels)) < 2:\n","                score = -1.0\n","            else:\n","                try:\n","                    score = silhouette_score(adata.obsm[rep_key], labels)\n","                except Exception:\n","                    score = -1.0\n","            if score > best_score:\n","                best_res, best_score, best_labels = res, score, labels\n","    else:\n","        for res in resolution_grid:\n","            labels = _cluster_at(res)\n","            try:\n","                score = silhouette_score(adata.obsm[rep_key], labels)\n","            except Exception:\n","                score = -1.0\n","            if best_clusters is None or best_clusters >= abs(\n","                    len(np.unique(labels)) - target_clusters):\n","                best_clusters = abs(len(np.unique(labels)) - target_clusters)\n","                best_res, best_score, best_labels = res, score, labels\n","\n","    labels = best_labels if best_labels is not None else _cluster_at(\n","        resolution_grid[0])\n","    adata.obs[dom_key] = labels\n","    adata.obs[dom_key] = adata.obs[dom_key].astype(\"category\")\n","    return labels, best_res, (best_score if return_silhouette else None)"],"metadata":{"id":"whqW3IZeT9PI","executionInfo":{"status":"ok","timestamp":1758315154231,"user_tz":420,"elapsed":4,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}}},"id":"whqW3IZeT9PI","execution_count":18,"outputs":[]},{"cell_type":"code","source":["config = Config(\n","    h5ad='data/1_visium.h5ad',\n","    dom_key = \"domain_scgpt\",\n","    rep_key = \"X_scGPT\",\n","    method = \"leiden\",\n","    target_clusters = 6,\n","    n_neighbors = 7,\n",")\n","\n","# run leiden for domain detection w. scGPT-spatial embeddings\n","labels, _, _ = predict_domain_using_embedding(adata, dom_key=config.dom_key,\n","                                              method=config.method,\n","                                              rep_key=config.rep_key,\n","                                              target_clusters=config.target_clusters,\n","                                              n_neighbors=config.n_neighbors)\n","\n","# save domain detection results to adata / csv.\n","adata.write(\"data/1_visium_domain_detection.h5ad\")\n","\n","print(adata)\n","print(adata.obs.domain_scgpt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"758L0_rJz95I","executionInfo":{"status":"ok","timestamp":1758315184838,"user_tz":420,"elapsed":30605,"user":{"displayName":"Sheryl Li","userId":"02842562777201031154"}},"outputId":"c00481f6-98de-4803-e2c1-54eb5b4afe34"},"id":"758L0_rJz95I","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["AnnData object with n_obs  n_vars = 4221  23325\n","    obs: 'in_tissue', 'array_row', 'array_col', 'Region', 'ground_truth', 'protocol', 'domain_scgpt_tmp', 'domain_scgpt'\n","    var: 'gene_ids', 'feature_types', 'genome', 'gene_name', 'id_in_vocab'\n","    uns: 'spatial', 'neighbors', 'domain_scgpt_tmp'\n","    obsm: 'spatial', 'X_scGPT'\n","    obsp: 'distances', 'connectivities'\n","AAACAACGAATAGTTC-1    0\n","AAACAAGTATCTCCCA-1    2\n","AAACAATCTACTAGCA-1    1\n","AAACACCAATAACTGC-1    3\n","AAACAGCTTTCAGAAG-1    4\n","                     ..\n","TTGTTGTGTGTCAAGA-1    1\n","TTGTTTCACATCCAGG-1    4\n","TTGTTTCATTAGTCTA-1    0\n","TTGTTTCCATACAACT-1    3\n","TTGTTTGTGTAAATTC-1    4\n","Name: domain_scgpt, Length: 4221, dtype: category\n","Categories (6, int8): [0, 1, 2, 3, 4, 5]\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python (.venv)","language":"python","name":".venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","collapsed_sections":["3cA4OTRRuPQN"],"runtime_attributes":{"runtime_version":"2025.07"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}